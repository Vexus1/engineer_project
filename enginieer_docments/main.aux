\relax 
\abx@aux@refcontext{none/global//global/global/global}
\abx@aux@cite{0}{mnih2015dqn}
\abx@aux@segm{0}{0}{mnih2015dqn}
\abx@aux@cite{0}{mnih2016a3c}
\abx@aux@segm{0}{0}{mnih2016a3c}
\abx@aux@cite{0}{samuel1959checkers}
\abx@aux@segm{0}{0}{samuel1959checkers}
\abx@aux@cite{0}{mitchell1997machinelearning}
\abx@aux@segm{0}{0}{mitchell1997machinelearning}
\abx@aux@cite{0}{HandsOnMachineLearning}
\abx@aux@segm{0}{0}{HandsOnMachineLearning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Wstęp}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Wprowadzenie do uczenia maszynowego}{3}{}\protected@file@percent }
\abx@aux@cite{0}{mnih2015nature}
\abx@aux@segm{0}{0}{mnih2015nature}
\abx@aux@cite{0}{sutton2018rl}
\abx@aux@segm{0}{0}{sutton2018rl}
\abx@aux@cite{0}{lapan2020deep}
\abx@aux@segm{0}{0}{lapan2020deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Podział uczenia maszynowego}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Uczenie nadzorowane}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Uczenie nienadzorowane}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Uczenie częściowo nadzorowane}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Uczenie przez wzmacnianie}{4}{}\protected@file@percent }
\abx@aux@cite{0}{wikipedia_reinforcement_learning}
\abx@aux@segm{0}{0}{wikipedia_reinforcement_learning}
\@writefile{toc}{\contentsline {section}{\numberline {3}Teoretyczne podstawy uczenia przez wzmacnianie}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Podstawowe pojęcia i definicje}{5}{}\protected@file@percent }
\abx@aux@cite{0}{sutton2018rl}
\abx@aux@segm{0}{0}{sutton2018rl}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Typowa struktura scenariusza uczenia przez wzmacnianie \blx@tocontentsinit {0}\cite {wikipedia_reinforcement_learning}}}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Modele Markowa (MDP)}{6}{}\protected@file@percent }
\abx@aux@cite{0}{sutton2018rl}
\abx@aux@segm{0}{0}{sutton2018rl}
\abx@aux@cite{0}{sutton2018rl}
\abx@aux@segm{0}{0}{sutton2018rl}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Równanie Bellmana}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Równanie Bellmana dla funkcji wartości stanu \( V_\pi (s) \)}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Równanie Bellmana dla funkcji wartości akcji \( Q_\pi (s,a) \)}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Równanie Bellmana dla polityki optymalnej \( V_*(s) \) i \( Q_*(s,a) \)}{9}{}\protected@file@percent }
\abx@aux@cite{0}{lapan2020deep}
\abx@aux@segm{0}{0}{lapan2020deep}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Metoda iteracji wartości}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Metoda iteracji polityki}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Metoda entropii krzyżowej w uczeniu przez wzmacnianie}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Twierdzenie o próbkowaniu istotnościowym}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Dywergencja Kullbacka-Leiblera}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementacjia wybranych algorytmów uczenia przez wzmacnianie}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Klasyfikacja algorytmów uczenia przez wzmacnianie}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Algorytmy optymalizujące wartości (Value optimization)}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Algorytmy optymalizujące politkę (Policy Optimization)}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Algorytmy imitacyjne (imitation)}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Wybór algorytmów do implementacji}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Dlaczego odrzucono klasyczną metodę Q-Learning?}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Deep Q-Learning (DQN)}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Architektura modelu}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Przykładowa architektura sieci Q dla DQN}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Proces treningu algorytmu DQN}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Zalety i wady DQN}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Advantage Actor-Critic (A2C)}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Architektura modelu}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Proces treningu algorytmu A2C}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Zaleti i wady A2C}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Eksperymenty i analiza wyników}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Konfiguracja środowiska testowego}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Środowisk testowe: Pong}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Charakterystyka środowiska Pong:}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Język programowania: Python}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Biblioteki wykorzystane do implementacji}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Kryteria oceny modeli}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Wyniki dla modelu Deep Q-Learning}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Analiza wykresu nagordy}{21}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Opis obrazka}}{21}{}\protected@file@percent }
\newlabel{fig:etykieta_obrazka}{{2}{21}{Analiza wykresu nagordy}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Opis implementacji modelu Deep Q-Learning}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Problem przetrenowania modelu}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Tabela z wynikami dla różnych hiperparametrów}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Wnioski}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Wyniki dla modelu Advantage Actor-Critic (A2C)}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Analiza wykresów dla modelu A2C}{24}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Opis obrazka}}{24}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Opis obrazka}}{25}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Opis obrazka}}{25}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Opis obrazka}}{26}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Opis obrazka}}{26}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Opis obrazka}}{27}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Opis obrazka}}{27}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Opis obrazka}}{28}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Opis obrazka}}{28}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Opis obrazka}}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Opis implementacji algorytmu A2C}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Tabela z wynikami dla różnych hiperparametrów}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4}Wnioski}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Bibliografia}{30}{}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{96936720C1126E2A62FD1C6D20E25634}
\abx@aux@defaultrefcontext{0}{mnih2015dqn}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mnih2016a3c}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{samuel1959checkers}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitchell1997machinelearning}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{HandsOnMachineLearning}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mnih2015nature}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sutton2018rl}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lapan2020deep}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wikipedia_reinforcement_learning}{none/global//global/global/global}
\gdef \@abspage@last{30}
